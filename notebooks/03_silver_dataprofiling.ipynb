{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f54e53",
   "metadata": {},
   "source": [
    "# 03 Silver data profiling\n",
    "- Load silver dataset from storage\n",
    "- Validate required training columns\n",
    "- Keep historical incidents only (Closed/Completed)\n",
    "- Remove duplicate and incomplete records\n",
    "- Profile timestamp quality, label balance, and text completeness\n",
    "- Export a compact profiling summary for downstream steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b3b7d",
   "metadata": {},
   "source": [
    "## 1) Config + load silver input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6669b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import io\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8eb4c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected buckets: ['incident-pipeline', 'incident-pipeline-test', 'mlflow-artifacts']\n"
     ]
    }
   ],
   "source": [
    "# Read .env for credentials\n",
    "def load_env_file(path: Path) -> dict:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing env file: {path.resolve()}\")\n",
    "\n",
    "    env = {}\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"=\", 1)\n",
    "        env[k.strip()] = v.strip()\n",
    "    return env\n",
    "\n",
    "# Env file location in repo\n",
    "ENV_FILE = Path(\"../docker/.env\")\n",
    "# Storage endpoint and config\n",
    "MINIO_ENDPOINT = \"localhost:9000\"\n",
    "MINIO_SECURE = False\n",
    "\n",
    "# Files locations in buckets\n",
    "SILVER_BUCKET = \"incident-pipeline-test\"\n",
    "SILVER_PREFIX = \"silver/incidents\"\n",
    "SILVER_OBJECT = f\"{SILVER_PREFIX}/incidents.parquet\"\n",
    "PROFILE_BUCKET = \"data-profile-test\"\n",
    "PROFILE_PREFIX = \"silver/incidents\"\n",
    "PROFILE_SUMMARY_OBJECT = f\"{PROFILE_PREFIX}/silver_dq_summary.json\"\n",
    "\n",
    "# Authenticate to file storage list buckets to confirm connection\n",
    "env = load_env_file(ENV_FILE)\n",
    "client = Minio(\n",
    "    MINIO_ENDPOINT,\n",
    "    access_key=env[\"MINIO_ROOT_USER\"],\n",
    "    secret_key=env[\"MINIO_ROOT_PASSWORD\"],\n",
    "    secure=MINIO_SECURE,\n",
    ")\n",
    "\n",
    "print(\"Connected buckets:\", [b.name for b in client.list_buckets()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41a5d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000\n",
      "Columns: ['sys_id', 'number', 'task_effective_number', 'sys_class_name', 'state', 'incident_state', 'priority', 'impact', 'urgency', 'severity', 'approval', 'escalation', 'notify', 'opened_at', 'resolved_at', 'closed_at', 'activity_due', 'due_date', 'sys_created_on', 'sys_updated_on', 'sys_created_by', 'sys_updated_by', 'opened_by', 'caller_id', 'resolved_by', 'closed_by', 'assignment_group', 'assigned_to', 'business_service', 'cmdb_ci', 'category', 'subcategory', 'contact_type', 'short_description', 'description', 'comments', 'work_notes', 'comments_and_work_notes', 'work_notes_list', 'parent_incident', 'problem_id', 'rfc', 'correlation_id', 'correlation_display', 'active', 'knowledge', 'made_sla', 'close_code', 'close_notes', 'calendar_duration', 'business_duration', 'calendar_stc', 'business_stc', 'sys_mod_count', 'reassignment_count', 'reopen_count', 'sla_due', 'sys_domain', 'sys_domain_path', 'watch_list', 'group_list', 'additional_assignee_list', 'u_system', 'u_system_criticality', 'u_initial_assignment_group', 'u_suggested_assignment_group', 'u_suggested_category', 'u_suggested_subcategory', 'u_outage_day', 'u_outage_system', 'bronze_run_id', 'ingested_at_utc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sys_id</th>\n",
       "      <th>number</th>\n",
       "      <th>task_effective_number</th>\n",
       "      <th>sys_class_name</th>\n",
       "      <th>state</th>\n",
       "      <th>incident_state</th>\n",
       "      <th>priority</th>\n",
       "      <th>impact</th>\n",
       "      <th>urgency</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>u_system</th>\n",
       "      <th>u_system_criticality</th>\n",
       "      <th>u_initial_assignment_group</th>\n",
       "      <th>u_suggested_assignment_group</th>\n",
       "      <th>u_suggested_category</th>\n",
       "      <th>u_suggested_subcategory</th>\n",
       "      <th>u_outage_day</th>\n",
       "      <th>u_outage_system</th>\n",
       "      <th>bronze_run_id</th>\n",
       "      <th>ingested_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f15a43b287b3f46cb189b5c147a9f458</td>\n",
       "      <td>INC1200546</td>\n",
       "      <td>INC1200546</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Closed</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>2 - High</td>\n",
       "      <td>2 - High</td>\n",
       "      <td>...</td>\n",
       "      <td>Core Network</td>\n",
       "      <td>High</td>\n",
       "      <td>Network Ops</td>\n",
       "      <td>Network Ops</td>\n",
       "      <td>Network</td>\n",
       "      <td>DNS</td>\n",
       "      <td>false</td>\n",
       "      <td></td>\n",
       "      <td>20260228T130029Z</td>\n",
       "      <td>2026-02-28 13:02:01.566995+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6f0c35505028c4a933934f9219a923e9</td>\n",
       "      <td>INC1205435</td>\n",
       "      <td>INC1205435</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Closed</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - High</td>\n",
       "      <td>1 - Critical</td>\n",
       "      <td>...</td>\n",
       "      <td>VPN Service</td>\n",
       "      <td>High</td>\n",
       "      <td>Network Ops</td>\n",
       "      <td>Network Ops</td>\n",
       "      <td>Network</td>\n",
       "      <td>VPN</td>\n",
       "      <td>false</td>\n",
       "      <td></td>\n",
       "      <td>20260228T130029Z</td>\n",
       "      <td>2026-02-28 13:02:01.566995+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8fcc0f613c48a29a0ad6e49bae3e8b32</td>\n",
       "      <td>INC1203952</td>\n",
       "      <td>INC1203952</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Closed</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>2 - High</td>\n",
       "      <td>...</td>\n",
       "      <td>Print Services</td>\n",
       "      <td>Low</td>\n",
       "      <td>End User Compute</td>\n",
       "      <td>End User Compute</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>Printing</td>\n",
       "      <td>false</td>\n",
       "      <td></td>\n",
       "      <td>20260228T130029Z</td>\n",
       "      <td>2026-02-28 13:02:01.566995+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             sys_id      number task_effective_number  \\\n",
       "0  f15a43b287b3f46cb189b5c147a9f458  INC1200546            INC1200546   \n",
       "1  6f0c35505028c4a933934f9219a923e9  INC1205435            INC1205435   \n",
       "2  8fcc0f613c48a29a0ad6e49bae3e8b32  INC1203952            INC1203952   \n",
       "\n",
       "  sys_class_name   state incident_state      priority      impact   urgency  \\\n",
       "0       Incident  Closed         Closed  3 - Moderate     3 - Low  2 - High   \n",
       "1       Incident  Closed         Closed  3 - Moderate  2 - Medium  2 - High   \n",
       "2       Incident  Closed         Closed  3 - Moderate     3 - Low   3 - Low   \n",
       "\n",
       "       severity  ...        u_system u_system_criticality  \\\n",
       "0      2 - High  ...    Core Network                 High   \n",
       "1  1 - Critical  ...     VPN Service                 High   \n",
       "2      2 - High  ...  Print Services                  Low   \n",
       "\n",
       "  u_initial_assignment_group u_suggested_assignment_group  \\\n",
       "0                Network Ops                  Network Ops   \n",
       "1                Network Ops                  Network Ops   \n",
       "2           End User Compute             End User Compute   \n",
       "\n",
       "  u_suggested_category u_suggested_subcategory u_outage_day u_outage_system  \\\n",
       "0              Network                     DNS        false                   \n",
       "1              Network                     VPN        false                   \n",
       "2             Hardware                Printing        false                   \n",
       "\n",
       "      bronze_run_id                  ingested_at_utc  \n",
       "0  20260228T130029Z 2026-02-28 13:02:01.566995+00:00  \n",
       "1  20260228T130029Z 2026-02-28 13:02:01.566995+00:00  \n",
       "2  20260228T130029Z 2026-02-28 13:02:01.566995+00:00  \n",
       "\n",
       "[3 rows x 72 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load silver parquet from storage\n",
    "resp = client.get_object(SILVER_BUCKET, SILVER_OBJECT)\n",
    "try:\n",
    "    silver_df = pd.read_parquet(io.BytesIO(resp.read()))\n",
    "finally:\n",
    "    resp.close()\n",
    "    resp.release_conn()\n",
    "\n",
    "# Print number of rows and sample of the data\n",
    "print(f\"Rows: {len(silver_df)}\")\n",
    "print(f\"Columns: {silver_df.columns.tolist()}\")\n",
    "silver_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b73a7",
   "metadata": {},
   "source": [
    "## 2) Training data filters + quality cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7c7c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns found\n"
     ]
    }
   ],
   "source": [
    "# Define required columns and check they are present in the data\n",
    "REQUIRED_COLS = [\"sys_id\", \"sys_updated_on\", \"short_description\", \"description\", \"active\", \"state\", \"assignment_group\"]\n",
    "MISSING_COLS = [col for col in REQUIRED_COLS if col not in silver_df.columns]\n",
    "if MISSING_COLS:\n",
    "    raise ValueError(f\"Missing required columns: {MISSING_COLS}\")\n",
    "print(\"All required columns found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f215a095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open records: 0\n",
      "Closed records: 10000\n",
      "Active=True records: 0\n",
      "Active=False records: 10000\n",
      "Filtered final record count: 10000\n"
     ]
    }
   ],
   "source": [
    "# Count open/closed and active true/false, then keep only closed/completed incidents\n",
    "state_norm = silver_df[\"state\"].astype(str).str.strip().str.lower()\n",
    "active_norm = silver_df[\"active\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "open_count = (state_norm == \"open\").sum()\n",
    "closed_count = (state_norm == \"closed\").sum()\n",
    "active_true_count = active_norm.isin([\"true\", \"1\", \"yes\", \"y\"]).sum()\n",
    "active_false_count = active_norm.isin([\"false\", \"0\", \"no\", \"n\"]).sum()\n",
    "\n",
    "print(f\"Open records: {open_count}\")\n",
    "print(f\"Closed records: {closed_count}\")\n",
    "print(f\"Active=True records: {active_true_count}\")\n",
    "print(f\"Active=False records: {active_false_count}\")\n",
    "\n",
    "keep_mask = state_norm.isin([\"closed\", \"completed\"])\n",
    "silver_filtered_df = silver_df.loc[keep_mask].copy()\n",
    "\n",
    "print(f\"Filtered final record count: {len(silver_filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bf86af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate sys_id rows removed: 0\n",
      "Record count after de-duplication: 10000\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate incidents by sys_id (keep first)\n",
    "duplicate_mask = silver_filtered_df.duplicated(subset=[\"sys_id\"], keep=\"first\")\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "silver_filtered_df = silver_filtered_df.loc[~duplicate_mask].copy()\n",
    "\n",
    "print(f\"Duplicate sys_id rows removed: {duplicate_count}\")\n",
    "print(f\"Record count after de-duplication: {len(silver_filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8bb884fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed for missing/blank required fields: 0\n",
      "Record count after required col cleanup: 10000\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with null/blank values in required training columns\n",
    "required_view = silver_filtered_df[REQUIRED_COLS].copy()\n",
    "null_or_blank = required_view.isna()\n",
    "\n",
    "# Treat whitespace only strings as missing for text/object fields\n",
    "for col in REQUIRED_COLS:\n",
    "    null_or_blank[col] = null_or_blank[col] | required_view[col].astype(str).str.strip().eq(\"\")\n",
    "\n",
    "rows_with_missing_required = null_or_blank.any(axis=1)\n",
    "missing_required_count = rows_with_missing_required.sum()\n",
    "silver_filtered_df = silver_filtered_df.loc[~rows_with_missing_required].copy()\n",
    "\n",
    "print(f\"Rows removed for missing/blank required fields: {missing_required_count}\")\n",
    "print(f\"Record count after required col cleanup: {len(silver_filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d007a",
   "metadata": {},
   "source": [
    "## 3) Profiling checks + output summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "554dbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training record count: 10000\n"
     ]
    }
   ],
   "source": [
    "# Final row count available for model training\n",
    "print(f\"Final training record count: {len(silver_filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cdcd701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_updated_on parse failures (training set): 0\n",
      "sys_updated_on min (training set): 2025-02-01 09:13:18+00:00\n",
      "sys_updated_on max (training set): 2026-01-31 23:58:32+00:00\n"
     ]
    }
   ],
   "source": [
    "# Parse timestamp field used for recency and split logic in later stages\n",
    "silver_filtered_df[\"_updated_dt\"] = pd.to_datetime(\n",
    "    silver_filtered_df[\"sys_updated_on\"], errors=\"coerce\", utc=True\n",
    ")\n",
    "\n",
    "# Report parse quality and observed time window after all filters\n",
    "failures = silver_filtered_df[\"_updated_dt\"].isna().sum()\n",
    "print(\"sys_updated_on parse failures (training set):\", failures)\n",
    "print(\"sys_updated_on min (training set):\", silver_filtered_df[\"_updated_dt\"].min())\n",
    "print(\"sys_updated_on max (training set):\", silver_filtered_df[\"_updated_dt\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00c2a4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment group distribution:\n",
      "  App Support - M365: 1762 / 17.62%\n",
      "  Network Ops: 1339 / 13.39%\n",
      "  App Support - Power BI: 1115 / 11.15%\n",
      "  App Support - Power Platform: 1025 / 10.25%\n",
      "  End User Compute: 976 / 9.76%\n",
      "  Identity and User Access: 875 / 8.75%\n",
      "  App Support - Microsoft Fabric: 833 / 8.33%\n",
      "  Security Operations: 615 / 6.15%\n",
      "  App Support - ERP: 407 / 4.07%\n",
      "  App Support - Finance: 380 / 3.8%\n",
      "  App Support - HRIS: 345 / 3.45%\n",
      "  Integration & Middleware: 328 / 3.28%\n"
     ]
    }
   ],
   "source": [
    "# Count label frequency to spot obvious class imbalance before modeling\n",
    "# Check label distribution of target variable (assignment_group)\n",
    "silver_filtered_df[\"assignment_group_clean\"] = (\n",
    "    silver_filtered_df[\"assignment_group\"].astype(str).str.strip()\n",
    ")\n",
    "label_counts = silver_filtered_df[\"assignment_group_clean\"].value_counts()\n",
    "total_records = len(silver_filtered_df)\n",
    "label_percentages = (label_counts / total_records * 100).round(2)\n",
    "print(\"Assignment group distribution:\")\n",
    "for group, count in label_counts.items():\n",
    "    percentage = label_percentages[group]\n",
    "    print(f\"  {group}: {count} / {percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f7b0389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes with < 20 examples: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignment_group_clean</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [count]\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flag rare classes for reporting\n",
    "MIN_CLASS_COUNT = 20  \n",
    "\n",
    "counts = silver_filtered_df[\"assignment_group_clean\"].value_counts(dropna=True)\n",
    "rare = counts[counts < MIN_CLASS_COUNT]\n",
    "\n",
    "print(f\"Classes with < {MIN_CLASS_COUNT} examples:\", len(rare))\n",
    "rare.to_frame(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6412e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty combined text rate: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean       411.844200\n",
       "std         95.289842\n",
       "min        116.000000\n",
       "50%        431.000000\n",
       "75%        452.000000\n",
       "90%        483.000000\n",
       "95%        503.050000\n",
       "max        570.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build combined text field and check for empty/short ticket content\n",
    "text = (\n",
    "    silver_filtered_df[\"short_description\"].fillna(\"\").astype(str).str.strip()\n",
    "    + \"\\n\" +\n",
    "    silver_filtered_df[\"description\"].fillna(\"\").astype(str).str.strip()\n",
    ").str.strip()\n",
    "\n",
    "empty_rate = text.eq(\"\").mean()\n",
    "lengths = text.str.len()\n",
    "\n",
    "print(\"Empty combined text rate:\", f\"{empty_rate:.2%}\")\n",
    "lengths.describe(percentiles=[0.5, 0.75, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c3cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket: data-profile-test\n",
      "Wrote summary to s3://data-profile-test/silver/incidents/silver_dq_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Save profiling summary to dedicated profiling bucket in object storage\n",
    "if not client.bucket_exists(PROFILE_BUCKET):\n",
    "    client.make_bucket(PROFILE_BUCKET)\n",
    "    print(f\"Created bucket: {PROFILE_BUCKET}\")\n",
    "else:\n",
    "    print(f\"Using existing bucket: {PROFILE_BUCKET}\")\n",
    "\n",
    "summary = {\n",
    "    \"run_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"silver_source\": {\"bucket\": SILVER_BUCKET, \"object\": SILVER_OBJECT},\n",
    "    \"rows_raw\": int(len(silver_df)),\n",
    "    \"rows_training\": int(len(silver_filtered_df)),\n",
    "    \"fields\": {\n",
    "        \"text\": [\"short_description\", \"description\"],\n",
    "        \"label\": \"assignment_group\",\n",
    "        \"label_clean\": \"assignment_group_clean\",\n",
    "    },\n",
    "    \"training_filters\": {\n",
    "        \"state_keep\": [\"closed\", \"completed\"],\n",
    "        \"dedupe\": {\"key\": \"sys_id\", \"keep\": \"first\"},\n",
    "        \"required_cols\": REQUIRED_COLS,\n",
    "    },\n",
    "    \"label_stats\": {\n",
    "        \"unique_classes\": int(silver_filtered_df[\"assignment_group_clean\"].nunique()),\n",
    "        \"min_class_count_reporting\": 20,\n",
    "        \"rare_classes\": rare.to_dict(),\n",
    "    },\n",
    "    \"time_window_training\": {\n",
    "        \"min\": str(silver_filtered_df[\"_updated_dt\"].min()),\n",
    "        \"max\": str(silver_filtered_df[\"_updated_dt\"].max()),\n",
    "        \"parse_failures\": int(silver_filtered_df[\"_updated_dt\"].isna().sum()),\n",
    "    },\n",
    "}\n",
    "\n",
    "summary_bytes = json.dumps(summary, indent=2).encode(\"utf-8\")\n",
    "client.put_object(\n",
    "    bucket_name=PROFILE_BUCKET,\n",
    "    object_name=PROFILE_SUMMARY_OBJECT,\n",
    "    data=io.BytesIO(summary_bytes),\n",
    "    length=len(summary_bytes),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "print(f\"Wrote summary to s3://{PROFILE_BUCKET}/{PROFILE_SUMMARY_OBJECT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
